{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a neural network using Tensorflow to classify text to identify clickbaits\n",
    "\n",
    "We will be using the Clickbait [data](https://github.com/bhargaviparanjape/clickbait) in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.102773Z",
     "start_time": "2022-04-30T18:57:24.096810Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.134810Z",
     "start_time": "2022-04-30T18:57:24.103809Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'data/clickbait_data', 'r', encoding='utf8') as f:\n",
    "    clickbait_data = f.readlines()\n",
    "\n",
    "with open(r'data/non_clickbait_data', 'r', encoding='utf8') as f:\n",
    "    non_clickbait_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing new line characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.166809Z",
     "start_time": "2022-04-30T18:57:24.136810Z"
    }
   },
   "outputs": [],
   "source": [
    "clickbait_data = [x.strip() for x in clickbait_data if x.strip()]\n",
    "non_clickbait_data = [x.strip() for x in non_clickbait_data if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.182810Z",
     "start_time": "2022-04-30T18:57:24.167811Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "# classes = ['clickbait', 'non_clickbait']\n",
    "# # Concatenate the lists to be added as dataframes\n",
    "# data = []\n",
    "# data.extend(clickbait_data)\n",
    "# data.extend(non_clickbait_data)\n",
    "# target = [classes.index('clickbait')] * len(clickbait_data)\n",
    "# target.extend([classes.index('non_clickbait')] * len(non_clickbait_data))\n",
    "\n",
    "# data = pd.DataFrame(data={'text': data, 'target': target})\n",
    "# data = data.sample(frac=1).reset_index(drop=True) # Shuffle the dataset\n",
    "\n",
    "# dataset = (\n",
    "#     tf.data.Dataset.from_tensor_slices(\n",
    "#         (\n",
    "#             data['text'].values,\n",
    "#             data['target'].values\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "# DATASET_SIZE = data.shape[0]\n",
    "\n",
    "# train_size = int(0.7 * DATASET_SIZE)\n",
    "# val_size = int(0.15 * DATASET_SIZE)\n",
    "# test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "# train_dataset = dataset.take(train_size)\n",
    "# test_dataset = dataset.skip(train_size)\n",
    "# val_dataset = test_dataset.skip(val_size)\n",
    "# test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "# assert len(train_dataset) + len(test_dataset) + len(val_dataset) == data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.326809Z",
     "start_time": "2022-04-30T18:57:24.183810Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# reference link: https://jonathan-hui.medium.com/tensorflow-dataset-data-preparation-b81fcf9c3c44\n",
    "classes = ['clickbait', 'non_clickbait']\n",
    "clickbait_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            clickbait_data,\n",
    "            [classes.index('clickbait')] * len(clickbait_data)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "non_clickbait_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            non_clickbait_data,\n",
    "            [classes.index('non_clickbait')] * len(non_clickbait_data)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = clickbait_dataset.concatenate(non_clickbait_dataset)\n",
    "dataset = dataset.shuffle(500)\n",
    "dataset = dataset.repeat(16).batch(32)\n",
    "DATASET_SIZE = len(dataset)\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "# assert len(train_dataset) + len(test_dataset) + len(val_dataset) == len(non_clickbait_data) + len(clickbait_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.342809Z",
     "start_time": "2022-04-30T18:57:24.327810Z"
    }
   },
   "outputs": [],
   "source": [
    "# for text in train_dataset:\n",
    "#     print(text)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualising samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.373809Z",
     "start_time": "2022-04-30T18:57:24.343809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_sample, label = iter(dataset).next()\n",
    "print(training_sample.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.389811Z",
     "start_time": "2022-04-30T18:57:24.375809Z"
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import string\n",
    "\n",
    "# def custom_standardization(input_data):\n",
    "#   lowercase = tf.strings.lower(input_data)\n",
    "#   stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "#   return tf.strings.regex_replace(stripped_html,\n",
    "#                                   '[%s]' % re.escape(string.punctuation),\n",
    "#                                   '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.405810Z",
     "start_time": "2022-04-30T18:57:24.391810Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "\n",
    "# max_features = 10000\n",
    "# sequence_length = 250\n",
    "\n",
    "# vectorize_layer = layers.TextVectorization(\n",
    "#     standardize=custom_standardization,\n",
    "#     max_tokens=max_features,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.421816Z",
     "start_time": "2022-04-30T18:57:24.406810Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Make a text-only dataset (without labels), then call adapt\n",
    "# train_text = train_dataset.map(lambda x, y: x)\n",
    "# # train_text = []\n",
    "# # for text in train_dataset:\n",
    "# #     x = text[0].numpy().decode('utf8')\n",
    "# #     train_text.append(x)\n",
    "# vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.436981Z",
     "start_time": "2022-04-30T18:57:24.422817Z"
    }
   },
   "outputs": [],
   "source": [
    "# def vectorize_text(text, label):\n",
    "#     text_str = tf.expand_dims(text, -1)\n",
    "# #     return vectorize_layer(text_str), tf.one_hot(label, depth=len(classes))\n",
    "#     return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.452996Z",
     "start_time": "2022-04-30T18:57:24.437982Z"
    }
   },
   "outputs": [],
   "source": [
    "# text_batch, label_batch = next(iter(train_dataset))\n",
    "# # print(text_batch, label_batch)\n",
    "# vectorize_text(text_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T08:29:43.858643Z",
     "start_time": "2022-04-25T08:29:43.828643Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.469018Z",
     "start_time": "2022-04-30T18:57:24.453995Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_ds = train_dataset.map(vectorize_text)\n",
    "# val_ds = val_dataset.map(vectorize_text)\n",
    "# test_ds = test_dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.485032Z",
     "start_time": "2022-04-30T18:57:24.470017Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.501057Z",
     "start_time": "2022-04-30T18:57:24.486031Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.517066Z",
     "start_time": "2022-04-30T18:57:24.503058Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# # 'embedding_dim'.\n",
    "# x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# # Conv1D + global max pooling\n",
    "# x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "# x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "# x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "# model = tf.keras.Model(inputs, predictions)\n",
    "# # opt = SGD(learning_rate=0.0001)\n",
    "# # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.533079Z",
     "start_time": "2022-04-30T18:57:24.518066Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#   layers.Embedding(max_features + 1, embedding_dim),\n",
    "#   layers.Dropout(0.2),\n",
    "#   layers.GlobalAveragePooling1D(),\n",
    "#   layers.Dropout(0.2),\n",
    "#   layers.Dense(1)])\n",
    "\n",
    "# print(model.summary())\n",
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer='adam',\n",
    "#               metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.549086Z",
     "start_time": "2022-04-30T18:57:24.534080Z"
    }
   },
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     batch_size=16,\n",
    "#     epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T14:46:00.156540Z",
     "start_time": "2022-04-30T14:46:00.142543Z"
    }
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T18:57:24.565103Z",
     "start_time": "2022-04-30T18:57:24.551086Z"
    }
   },
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\"\n",
    "\n",
    "tfhub_handle_preprocess=\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:21.841700Z",
     "start_time": "2022-05-01T00:21:21.825699Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.25)(net)\n",
    "  net = tf.keras.layers.Dense(1, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:36.531735Z",
     "start_time": "2022-05-01T00:21:22.605748Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:36.547735Z",
     "start_time": "2022-05-01T00:21:36.533735Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:36.562735Z",
     "start_time": "2022-05-01T00:21:36.548735Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 5e-5\n",
    "# init_lr = 7e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:37.735664Z",
     "start_time": "2022-05-01T00:21:37.718666Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"model/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T00:21:50.297785Z",
     "start_time": "2022-05-01T00:21:50.276788Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "#                          loss=loss,\n",
    "                         loss=\"binary_crossentropy\",\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T06:38:25.906978Z",
     "start_time": "2022-05-01T00:22:02.272436Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_dataset,\n",
    "                               validation_data=val_dataset,\n",
    "#                                steps_per_epoch=100,\n",
    "#                                validation_steps=100,\n",
    "                               epochs=epochs,\n",
    "                               batch_size=32,\n",
    "                               verbose=2,\n",
    "                               callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
